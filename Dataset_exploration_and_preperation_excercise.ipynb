{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPALtHn5/pAQ21ivmahi+5M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slkh/COVIDPause/blob/master/Dataset_exploration_and_preperation_excercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This excercise will give a prespective on data prepration for a simple morphological segmentation task.\n",
        "\n",
        "There are two main goals in this excercise: \n",
        "- Prepare and explore data\n",
        "- Get familiar with unix commands\n",
        "\n",
        "In a larger project, you will likely need to write a proper script to deal with the data and prepare it. But sometimes you want to explore the data and get quick counts for different scenarios before fully comitting to using the data. This is what we are doing in this excercise.\n",
        "\n",
        "--- "
      ],
      "metadata": {
        "id": "gIuyJV0LTWgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we are using comes from a [morphological segmentation shared task](https://github.com/sigmorphon/2022SegmentationST).\n",
        "\n",
        "\n",
        "_❓ TL;DR: A shared task is compition where you have a defined task and data that is prepared specifically for that task and ask people to submit systems that solve the given task at high accuracies._\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8PFFYT-cUM38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "First, let's clone the repo containing the data:"
      ],
      "metadata": {
        "id": "ekH0hEfBWcXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i03BhBfkS7N5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sigmorphon/2022SegmentationST.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's navigate to where the data is sitting:\n",
        "\n",
        "1. view the contents of the main directory: "
      ],
      "metadata": {
        "id": "FdMqqqbAXTgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l 2022SegmentationST/"
      ],
      "metadata": {
        "id": "TN7vEcVTTCL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Navigate to the data folder _and_ list its contents:"
      ],
      "metadata": {
        "id": "taIi3dyFX8uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 2022SegmentationST/data/\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "uIDFPx5GX4_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Explore data files\n",
        "We can observe the following:\n",
        "- There are files for 9 languages: `ces`, `eng`, `fra`, `hun`, `ita`, `lat`, `mon`, `rus`, and `spa`.\n",
        "- For some languages we have two types of data sets: `sentences`: running natural langauge (text), and `word`: word types.\n",
        "- For each langugae and each type the data is split in to three standard splits: `train`, `dev`, `test`.\n",
        "- All the files are in `.tsv` format, meaning that the values in one line are tab seperated.\n",
        "- `test` has two files: one with `gold` and one without."
      ],
      "metadata": {
        "id": "qPDEVcK6cIQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For conveniance, we will work with English data -- labled `eng`.\n",
        "Since we want to work with more naturalistic data, we will look at the contents of the `sentence` files:"
      ],
      "metadata": {
        "id": "BuVoeP4jfYTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "sx6-_GcscH8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command `head` shows the first 10 lines of a given file. If we look carfully at the first line, we can see that there are two sentences (note that the lines wrap when they are dispalyed).\n",
        "\n",
        "Let's look more carefully:"
      ],
      "metadata": {
        "id": "uMg46lYNfzcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the very first line of the file only:\n",
        "!head -1 eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "5X0Vt02EYEw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the following command `cut` literally \"cuts\" the line according to a delimenter, but default a tab `\\t`\n",
        "!head -1 eng.sentence.train.tsv|cut -f1"
      ],
      "metadata": {
        "id": "J9KhQM1OB4t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -1 eng.sentence.train.tsv|cut -f2"
      ],
      "metadata": {
        "id": "Ja0RBJalCRaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the first coulmn (after running `cut -f1`) is the _whitespace_ tokenized sentence, while the second one (`cut -f2`) contains the segmented words. According to the documentation of the data `@@` indicates the morpheme. Any morpheme with `@@` will attach to what's to the left of it.\n",
        "\n",
        "We now know that each `sentence` file will have the format of:\n",
        "\n",
        "```\n",
        "Whitespace tokenized sentence \\t White @@space token @@ize @@ed sentence\n",
        "```\n",
        "\n",
        "Let see how many sentence are there in each `eng.sentence.*.tsv`:"
      ],
      "metadata": {
        "id": "_rx8-KlUCNUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to view counts we run the `wc` command, to look at \"line\" numbers only we use the \"-l\" option\n",
        "!wc -l eng.sentence.*.tsv"
      ],
      "metadata": {
        "id": "Y-EQqyriCL5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that both `dev` and `test` have similar counts of sentences, while  `train` has much more sentences, this makes sense because we usually need more data to _train_ a system.\n",
        "\n",
        "Additionally, we see that there are two files for `test`, one of them has `gold` in the name. This basically means that the file has the answers (aka the segmentations). This is because in shared tasks, the `test` file is distributed at the very end of the compition to fairly evaluate the submitted systems. The _answers_ are usually revealed after the shared task comes to an end. To see the difference, let's look at the first line from both files:"
      ],
      "metadata": {
        "id": "oXV6qpUUG8_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -1 eng.sentence.test.tsv"
      ],
      "metadata": {
        "id": "owGCLL3RGp-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -1 eng.sentence.test.gold.tsv"
      ],
      "metadata": {
        "id": "-0hU0NUnWN0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our purposes we will be using the following files:\n",
        "```\n",
        "eng.sentence.dev.tsv\n",
        "eng.sentence.test.gold.tsv\n",
        "eng.sentence.train.tsv\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "1QgWaBQxWV0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Processing data\n",
        "Let's create our own directory and copy the data files that we are going to use to the new directory.\n",
        "\n",
        "First, let's go to the _root_ directory:"
      ],
      "metadata": {
        "id": "KuqzdP_TWmtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to view where we are now, we run the command:\n",
        "!pwd"
      ],
      "metadata": {
        "id": "9hLmzxYpWRnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we go two levels \"up\":\n",
        "%cd ../../"
      ],
      "metadata": {
        "id": "_55uGZhWXELu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a new **dir**ectory (aka folder) and name is `stats`, we then navigate into `stats`"
      ],
      "metadata": {
        "id": "C09Fr4aiXXp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir stats"
      ],
      "metadata": {
        "id": "h8vQm6YIXTYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stats"
      ],
      "metadata": {
        "id": "LJ4FhX3GXfMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's copy the files we are going to use to our directory:"
      ],
      "metadata": {
        "id": "WaRkzweAZBHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note the use of the wildcard \"*\" to mean any character\n",
        "%cp ../2022SegmentationST/data/eng.sentence.*.tsv ."
      ],
      "metadata": {
        "id": "-VmgrmwFY_ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what files do we have:\n",
        "! ls -l"
      ],
      "metadata": {
        "id": "NX6r3ODwZVOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we don't need `eng.sentence.test.tsv`, we will remove it.\n",
        "\n",
        "**WARNING: don't use the following command -in general- unless you are really sure about what you are doing, there is absolutely NO GOING BACK from this command!**"
      ],
      "metadata": {
        "id": "f9b30qNyZkIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm eng.sentence.test.tsv"
      ],
      "metadata": {
        "id": "qmIadZu6ZftB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check on the files again:\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "C29H2JTtZ0g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, for each file, let's create a file with two columns where one has the \"raw\" word and the other has the segmented word. There are many ways to do this, but here let's do it using simple commands to get familiar with the different core commands.\n",
        "\n",
        "First, we take the unsegmented sentence and make it a list of words instead, let's test it on the first line only and then apply it to the entire file:"
      ],
      "metadata": {
        "id": "0JrhLpNXZ72f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the command `cat` is intended for concatinating files, but it can also be used\n",
        "# to just print out files (despite what unix prescriptivits think)\n",
        "!cat eng.sentence.train.tsv| head -1|cut -f1"
      ],
      "metadata": {
        "id": "UY5csXfyZ3yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to turn a sentence into a list of words we are replacing each whitespace with \n",
        "# a new line. We are using perl regular expressions (regex) to do this:\n",
        "!cat eng.sentence.train.tsv| head -1|cut -f1|perl -pe 's/ /\\n/g;'"
      ],
      "metadata": {
        "id": "ZJxX2mNLbEg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we now do this to the entire file by removing `head -1` from the previous \n",
        "# command. We then look at the number of \"lines\" which are word tokens in this case:\n",
        "!cat eng.sentence.train.tsv|cut -f1|perl -pe 's/ /\\n/g;'|wc -l"
      ],
      "metadata": {
        "id": "gELUghMjbbMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's store what we did into a file using \" > \"file_name\"\n",
        "!cat eng.sentence.train.tsv|cut -f1|perl -pe 's/ /\\n/g;' > train_raw"
      ],
      "metadata": {
        "id": "ool6Z2Nubsw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You many now be tempted to do the same thing to get the list of the segmented words, which is very reasonable, but let's not forget that the segmentations in their current format are space seperated, and if we try to align both word lists we will get a mismatch. So first, let's attach those segments to their words. We will do this for the first line to show an example and then apply it to the entire file:"
      ],
      "metadata": {
        "id": "gxJ7bswEcDNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to replace the space to the left of every `@@` with \"nothing\"\n",
        "!cat eng.sentence.train.tsv|cut -f2|head -1"
      ],
      "metadata": {
        "id": "Ss85EC9vcBIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the regex basically says replace every \" @@\" with \"@@\", which translates to \n",
        "# remove whitespace before \"@@\"\n",
        "!cat eng.sentence.train.tsv|cut -f2|head -1|perl -pe 's/ @@/@@/g;'"
      ],
      "metadata": {
        "id": "DD-Xy9xUc37K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What if we want to replace \"@\"s with \"+\" sign so it is more readable and easier to debug?\n",
        "!cat eng.sentence.train.tsv|cut -f2|head -1|perl -pe 's/ @@/@@/g;'|perl -pe 's/@@/+/g;'"
      ],
      "metadata": {
        "id": "2DuqJRQNdHON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's compress the regex\n",
        "!cat eng.sentence.train.tsv|cut -f2|head -1|perl -pe 's/ @@/+/g;'"
      ],
      "metadata": {
        "id": "sFUF28wwddz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to deal with segments, we can use the same command to turn a sentence into a list:"
      ],
      "metadata": {
        "id": "TrovnjVAd7tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we are using the group match (\\S) just to make sure that the @@ is followed\n",
        "# by a string an not a standing alone symbol. To retain what we matched we capture\n",
        "# the group with \"$1\"\n",
        "!cat eng.sentence.train.tsv|cut -f2|perl -pe 's/ @@(\\S)/+$1/g; s/ /\\n/g;' > train_seg"
      ],
      "metadata": {
        "id": "YJ-z5oDYd3tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's make sure that both words lists have the same number of lines:"
      ],
      "metadata": {
        "id": "dQqOwY56vADZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l train_*"
      ],
      "metadata": {
        "id": "qe9Epa0ovKvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with issue found in the data\n",
        "Uh-Oh! we see that there is a discrepancy in the number of lines, the list of segmentations has 2 extra tokens, or the raw has 2 less tokens. One quick way to see what's happening is for look for the differences between the two lists (other than the segmented words), chances are there is where the issue lies:"
      ],
      "metadata": {
        "id": "hLPb3zY2vVPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use the \"sdiff\" command, and we will supress the similar entries and \n",
        "# those with segmentation\n",
        "!sdiff -s train_raw train_seg|egrep -v \"\\+\""
      ],
      "metadata": {
        "id": "luHbO36GvR1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main culpert seems to be two additional words in the segmentation side, `Caucasian` and `West`. Let's look for those sentences in the original files: "
      ],
      "metadata": {
        "id": "e23UXjj3wH65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the option -n prints the line number\n",
        "!egrep -n 'Caucasian' eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "vvxodsBAwXj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see there are two issues here, the main one is that there is an extra word in the segmentation side and a change of capitalization as well.\n",
        "\n",
        "Let's now look for the other issue, the word \"West\" is likely to be more frequent than \"Caucasian\", so first we will see how many sentences contain it and then we will try to narrow down the sentence of interest:"
      ],
      "metadata": {
        "id": "1IMUX9e0xZzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!egrep -n 'West' eng.sentence.train.tsv|wc -l"
      ],
      "metadata": {
        "id": "xx2-ttIhx31Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We know that the words Hammer in the \"raw\" side is present, so let's look for\n",
        "# a line that contains both\n",
        "!egrep -n 'Hammer.*West' eng.sentence.train.tsv|wc -l"
      ],
      "metadata": {
        "id": "FnPdrF_IyMuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Great, only one sentence\n",
        "!egrep -n 'Hammer.*West' eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "FnHZb-aBycuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have the exact same issue."
      ],
      "metadata": {
        "id": "pvw1M8R5yjW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are multiple ways to deal with this, the sensible thing to do in this case is to _ignore_ those two sentence and not include them in our data. The last thing we want to do is to go in and try to \"fix it\". The common practice in shared tasks is to immediatly inform the organizers of the issue and they are usually very responsive and will fix the issues or will let you know how to deal with it. Until the data is fixed, we usually ignore the ill formed entires and continue with our work so we are not stuck.\n",
        "\n",
        "Since we are working with a *_copy_* of the data, we will add an indicator at the beginning of the lines to helps quickly identify them."
      ],
      "metadata": {
        "id": "wzzO0N8X4s92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sed is very powerful, and it is worth learning if you interested in this kind of\n",
        "# scripting. Here we will be adding 3 \"#\" in front of the problematic lines.\n",
        "!sed -i -e '1212s/^/### /' -e '6502s/^/### /' eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "YwgFuO6B8Nct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's quickly check the file for those lines\n",
        "!egrep -n '(Hammer.*West|Caucasian)' eng.sentence.train.tsv"
      ],
      "metadata": {
        "id": "vsSE4CUi8um0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's repeate the process of creating the lists again and check the line numbers to double check. Except this time we will make sure to ignore the lines that we just marked:"
      ],
      "metadata": {
        "id": "lLFnLfrD9D4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat eng.sentence.train.tsv|egrep -v \"^### \"|cut -f1|perl -pe 's/ /\\n/g;' > train_raw\n",
        "!cat eng.sentence.train.tsv|egrep -v \"^### \"|cut -f2|perl -pe 's/ @@(\\S)/+$1/g; s/ /\\n/g;' > train_seg\n",
        "!wc -l train_*"
      ],
      "metadata": {
        "id": "L2MjEixL9U9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesomesauce! Now let's create a file with the two lists next to each other seperated by a tab `\\t`:"
      ],
      "metadata": {
        "id": "Bj6PhRHReT5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the command `paste` does that job\n",
        "!paste train_raw train_seg > train_list"
      ],
      "metadata": {
        "id": "GfTm_NcuePJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first few words\n",
        "!head train_list"
      ],
      "metadata": {
        "id": "CHjwmaS5eiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And the last few words to make sure they properly aligned\n",
        "!tail train_list"
      ],
      "metadata": {
        "id": "6iWjrNBT9oRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# triple check the number of lines for all the word lists\n",
        "!wc -l train_*"
      ],
      "metadata": {
        "id": "BNcBXc9tmOiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amaizing! Now let's do that for both `dev` and `test`, including the sanity checks:"
      ],
      "metadata": {
        "id": "UnQZozP5eqLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dev\n",
        "!cat eng.sentence.dev.tsv|cut -f1|perl -pe 's/ /\\n/g;' > dev_raw\n",
        "!cat eng.sentence.dev.tsv|cut -f2|perl -pe 's/ @@(\\S)/+$1/g; s/ /\\n/g;' > dev_seg\n",
        "!wc dev_*"
      ],
      "metadata": {
        "id": "C9PlaUNheoeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!paste dev_raw dev_seg > dev_list"
      ],
      "metadata": {
        "id": "Q1GXaqux-Kpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head dev_list"
      ],
      "metadata": {
        "id": "qoZMPFk1CjH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail dev_list"
      ],
      "metadata": {
        "id": "5eocIqkwCnYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc dev_*"
      ],
      "metadata": {
        "id": "rhNVhP41CsT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "!cat eng.sentence.test.gold.tsv|cut -f1|perl -pe 's/ /\\n/g;' > test_raw\n",
        "!cat eng.sentence.test.gold.tsv|cut -f2|perl -pe 's/ @@(\\S)/+$1/g; s/ /\\n/g;' > test_seg\n",
        "!wc test_*"
      ],
      "metadata": {
        "id": "_dYjSbigfAXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!paste test_raw test_seg > test_list"
      ],
      "metadata": {
        "id": "g-ppDZwsCx-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head test_list"
      ],
      "metadata": {
        "id": "E1v_AEVBC_Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail test_list"
      ],
      "metadata": {
        "id": "-hZAnoumDDKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wc test_*"
      ],
      "metadata": {
        "id": "8TSO_e8_DGgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick check of the current contents:\n",
        "!ls -l"
      ],
      "metadata": {
        "id": "Cw0Fn_dJfCtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the number of lines in both the original sentence files and our new files\n",
        "!wc -l eng.* *_list"
      ],
      "metadata": {
        "id": "6qfVpvsZfVov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These numbers makese sense and the differences are consistent between the lists and the files."
      ],
      "metadata": {
        "id": "HFZwFD1wfnIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokens vs Types\n",
        "Since we are not using full sentences for \"our task\" and we are just using pairs of input and output, i.e., `raw` and `seg`, we really don't need the \"repetion\" of the pairs, one example should be enough. For example, a system should not need to see `said --> say+ed` 10 times before it \"learns\" how to do it.\n",
        "\n",
        "_❓ TL;DR: `tokens` refer to the indivisual instances words in a text, while `types` are the unique tokens. For example, in this list: `bad,bad,bad,sad,sad,rad` there are 6 tokens and 3 types._\n",
        "\n",
        "Now let's reduce our lists to types."
      ],
      "metadata": {
        "id": "MxHCTo26ODcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several way to get the list of \"unique\" lines from a file, in this excercise we are going to use a combination of `sort` and `uniq`."
      ],
      "metadata": {
        "id": "OVETWh56XIYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will start with train_list, first check the number of lines:\n",
        "!wc -l train_list"
      ],
      "metadata": {
        "id": "6od1ztfmWnMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we sort the file and then unique the lines, and then check the number of lines:\n",
        "!sort train_list|uniq|wc"
      ],
      "metadata": {
        "id": "ZLBld7a_XhaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets save the types into a new file\n",
        "!sort train_list|uniq > train_list.types"
      ],
      "metadata": {
        "id": "xzDpabIKX7P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is huge difference between the number of tokens and types, which is expected since this extracted from running text. Now, what if we want to keep the information about how frequent each type is? Very simple:"
      ],
      "metadata": {
        "id": "TXNVVrqPX21I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the option -c, we can additionally sort them by frequency and look at\n",
        "# the top ten most frequent types\n",
        "!sort train_list|uniq -c|sort -nr|head"
      ],
      "metadata": {
        "id": "Dq30i-5rX2Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get a frequency list\n",
        "\n",
        "This makes a lot of sense! Now lets save the frequency list for reference"
      ],
      "metadata": {
        "id": "H57X3i0qYhwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sort train_list|uniq -c|sort -nr > train_list.tpyes_freq"
      ],
      "metadata": {
        "id": "Pbn21rxQYyu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create the same lists for both `dev` and `train`"
      ],
      "metadata": {
        "id": "W-UeZSESY6d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dev\n",
        "!sort dev_list|uniq > dev_list.types\n",
        "!sort dev_list|uniq -c|sort -nr > dev_list.tpyes_freq\n",
        "# test\n",
        "!sort test_list|uniq > test_list.types\n",
        "!sort test_list|uniq -c|sort -nr > test_list.tpyes_freq"
      ],
      "metadata": {
        "id": "Suy6hzD9Y-Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quickly peek at the number of lines for all the .types files\n",
        "!wc *.types"
      ],
      "metadata": {
        "id": "ExFbOp3dZNtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Create OOV splits\n",
        "\n",
        "At this stage, the files are technically ready for training a segmentor and even evaluating it. However, in tasks such as morphological segmentation, systems must generalize well for unseen word. \n",
        "\n",
        "_❓ TL;DR: unseen words or tokens are those tokens that are not present in the dataset used for training a system, in our case `train`. They are also called Out-of-Vocabulary (OOV)_\n",
        "\n",
        "Most NLP tasks deal with _natural_ langugage and therefore it is normal to see some overlaps between {`dev`, `test`} and `train`. What is not acceptable is training on `dev` and `test`. In other words, we should not training our system on the same datasets that we are going to evaluate it on, it is not fair and will give very skewed and biased resutls, and most importantly it will not be able to generalize!\n",
        "In many cases were we want to test the limits of system generalization we keep only unseen (or OOV) instances in both `dev` and `test`. This is what we will do next."
      ],
      "metadata": {
        "id": "isnIg94wqOgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the `comm` command to find the common lines or the lack thereof between two files. `comm` works with sorted files, and since our files were generated by sorting them, then we don't need to re-sort them again."
      ],
      "metadata": {
        "id": "dVoFo8BSa5br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the OOVs in dev, and how many they are\n",
        "!comm -13 train_list.types dev_list.types|wc"
      ],
      "metadata": {
        "id": "_wNewShLmNPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save them to a new file\n",
        "!comm -13 train_list.types dev_list.types > dev_list_oov.types"
      ],
      "metadata": {
        "id": "Vu7dUQNPbZle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same for test\n",
        "!comm -13 train_list.types test_list.types > test_list_oov.types"
      ],
      "metadata": {
        "id": "f_gpmqxRblhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we do this only for `dev` and `test` because it is OOV with respect to `train`.\n",
        "\n",
        "Now let's look at the overall counts of our types:"
      ],
      "metadata": {
        "id": "vA2jbTNPbsJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l *.types"
      ],
      "metadata": {
        "id": "4GQcPvARVfnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For both `dev` and `test` the OOV types are around 30% of the whole set."
      ],
      "metadata": {
        "id": "Eq-hy9ggeey8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You have now explored and prepared data for your upcoming segmentation task!"
      ],
      "metadata": {
        "id": "dW8pqAx87gEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "In this excercise we learned how to explore data and quickly create lists and splits and inspect them using core unix commands. If we were training a segmentation system our data is technically ready for training, developing, and evaluting. However, as we start working with the data more, we may discover bugs, or want to tweak how we generated some lists. Therefore, it is wise to write a proper script (e.g. in python) that will produce the same files that we produced using the command line. This is cleaner for running the pipeline for as many times as we need without worrying about accidental error propagation.\n",
        "\n",
        "## Tips\n",
        "- If you want to know more about any of the commands we used just type `man` followed by the command to get the manual on the command.\n",
        "- If you want to run the commands locally on your terminal, you need to remove the `!` and `%` symbols we have here.\n",
        "- If this notebook is not mounted on your drive or is not local on your machine, all your progress will be lost if the runtime is disconnected.\n",
        "\n",
        "## Bonus Excercise\n",
        "\n",
        "Create a python script that will generate the list files, types, type frequencies, and OOV types. Compare the output with what we got here and see if there any more issues with the data. "
      ],
      "metadata": {
        "id": "5fa1aR6weswL"
      }
    }
  ]
}